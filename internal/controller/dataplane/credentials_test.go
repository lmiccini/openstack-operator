/*

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package dataplane

import (
	"context"
	"encoding/json"
	"slices"
	"testing"
	"time"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/controller-runtime/pkg/client/fake"

	"github.com/openstack-k8s-operators/lib-common/modules/common/secret"
	dataplanev1 "github.com/openstack-k8s-operators/openstack-operator/api/dataplane/v1beta1"
)

func TestGetNodesCoveredByDeployment(t *testing.T) {
	tests := []struct {
		name            string
		ansibleLimit    string
		nodesetNodes    []string
		expectedCovered []string
	}{
		{
			name:            "nil nodeset returns empty",
			ansibleLimit:    "",
			nodesetNodes:    nil,
			expectedCovered: []string{},
		},
		{
			name:            "empty AnsibleLimit returns all nodes",
			ansibleLimit:    "",
			nodesetNodes:    []string{"compute-0", "compute-1"},
			expectedCovered: []string{"compute-0", "compute-1"},
		},
		{
			name:            "wildcard returns all nodes",
			ansibleLimit:    "*",
			nodesetNodes:    []string{"compute-0", "compute-1", "compute-2"},
			expectedCovered: []string{"compute-0", "compute-1", "compute-2"},
		},
		{
			name:            "exact match returns single node",
			ansibleLimit:    "compute-0",
			nodesetNodes:    []string{"compute-0", "compute-1", "compute-2"},
			expectedCovered: []string{"compute-0"},
		},
		{
			name:            "comma-separated returns multiple nodes",
			ansibleLimit:    "compute-0,compute-2",
			nodesetNodes:    []string{"compute-0", "compute-1", "compute-2"},
			expectedCovered: []string{"compute-0", "compute-2"},
		},
		{
			name:            "wildcard pattern returns matching nodes",
			ansibleLimit:    "compute-*",
			nodesetNodes:    []string{"compute-0", "compute-1", "other-node"},
			expectedCovered: []string{"compute-0", "compute-1"},
		},
		{
			name:            "mixed exact and wildcard",
			ansibleLimit:    "compute-0,other-*",
			nodesetNodes:    []string{"compute-0", "compute-1", "other-node", "other-2"},
			expectedCovered: []string{"compute-0", "other-node", "other-2"},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Create deployment
			var deployment *dataplanev1.OpenStackDataPlaneDeployment
			if tt.nodesetNodes != nil {
				deployment = &dataplanev1.OpenStackDataPlaneDeployment{
					Spec: dataplanev1.OpenStackDataPlaneDeploymentSpec{
						AnsibleLimit: tt.ansibleLimit,
					},
				}
			}

			// Create nodeset
			var nodeset *dataplanev1.OpenStackDataPlaneNodeSet
			if tt.nodesetNodes != nil {
				nodes := make(map[string]dataplanev1.NodeSection)
				for _, nodeName := range tt.nodesetNodes {
					nodes[nodeName] = dataplanev1.NodeSection{}
				}
				nodeset = &dataplanev1.OpenStackDataPlaneNodeSet{
					Spec: dataplanev1.OpenStackDataPlaneNodeSetSpec{
						Nodes: nodes,
					},
				}
			}

			// Call function
			covered := getNodesCoveredByDeployment(deployment, nodeset)

			// Check result
			if len(covered) != len(tt.expectedCovered) {
				t.Errorf("getNodesCoveredByDeployment() returned %d nodes, want %d", len(covered), len(tt.expectedCovered))
			}

			// Check each expected node is covered
			for _, expectedNode := range tt.expectedCovered {
				found := false
				for _, node := range covered {
					if node == expectedNode {
						found = true
						break
					}
				}
				if !found {
					t.Errorf("getNodesCoveredByDeployment() missing expected node %q", expectedNode)
				}
			}
		})
	}
}

func TestGetAllNodeNames(t *testing.T) {
	tests := []struct {
		name          string
		nodeset       *dataplanev1.OpenStackDataPlaneNodeSet
		expectedNodes []string
	}{
		{
			name:          "nil nodeset returns empty",
			nodeset:       nil,
			expectedNodes: []string{},
		},
		{
			name: "single node",
			nodeset: &dataplanev1.OpenStackDataPlaneNodeSet{
				Spec: dataplanev1.OpenStackDataPlaneNodeSetSpec{
					Nodes: map[string]dataplanev1.NodeSection{
						"compute-0": {},
					},
				},
			},
			expectedNodes: []string{"compute-0"},
		},
		{
			name: "multiple nodes",
			nodeset: &dataplanev1.OpenStackDataPlaneNodeSet{
				Spec: dataplanev1.OpenStackDataPlaneNodeSetSpec{
					Nodes: map[string]dataplanev1.NodeSection{
						"compute-0": {},
						"compute-1": {},
						"compute-2": {},
					},
				},
			},
			expectedNodes: []string{"compute-0", "compute-1", "compute-2"},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			nodes := getAllNodeNames(tt.nodeset)

			if len(nodes) != len(tt.expectedNodes) {
				t.Errorf("getAllNodeNames() returned %d nodes, want %d", len(nodes), len(tt.expectedNodes))
			}

			// Check each expected node exists
			for _, expectedNode := range tt.expectedNodes {
				found := false
				for _, node := range nodes {
					if node == expectedNode {
						found = true
						break
					}
				}
				if !found {
					t.Errorf("getAllNodeNames() missing expected node %q", expectedNode)
				}
			}
		})
	}
}

func TestComputeDeploymentSummary(t *testing.T) {
	tests := []struct {
		name               string
		trackingData       *SecretTrackingData
		totalNodes         int
		expectedUpdated    int
		expectedAllUpdated bool
	}{
		{
			name: "all nodes updated",
			trackingData: &SecretTrackingData{
				NodeStatus: map[string]NodeSecretStatus{
					"node1": {AllSecretsUpdated: true},
					"node2": {AllSecretsUpdated: true},
					"node3": {AllSecretsUpdated: true},
				},
			},
			totalNodes:         3,
			expectedUpdated:    3,
			expectedAllUpdated: true,
		},
		{
			name: "partial nodes updated",
			trackingData: &SecretTrackingData{
				NodeStatus: map[string]NodeSecretStatus{
					"node1": {AllSecretsUpdated: true},
					"node2": {AllSecretsUpdated: false},
					"node3": {AllSecretsUpdated: true},
				},
			},
			totalNodes:         3,
			expectedUpdated:    2,
			expectedAllUpdated: false,
		},
		{
			name: "no nodes updated",
			trackingData: &SecretTrackingData{
				NodeStatus: map[string]NodeSecretStatus{
					"node1": {AllSecretsUpdated: false},
					"node2": {AllSecretsUpdated: false},
				},
			},
			totalNodes:         2,
			expectedUpdated:    0,
			expectedAllUpdated: false,
		},
		{
			name: "empty tracking data",
			trackingData: &SecretTrackingData{
				NodeStatus: map[string]NodeSecretStatus{},
			},
			totalNodes:         0,
			expectedUpdated:    0,
			expectedAllUpdated: false,
		},
		{
			name: "drift detected - updatedNodes reset to 0",
			trackingData: &SecretTrackingData{
				Secrets: map[string]SecretVersionInfo{
					"secret1": {
						CurrentHash:      "hash-v1",
						ExpectedHash:     "hash-v2", // Drift!
						NodesWithCurrent: []string{"node1", "node2"},
					},
				},
				NodeStatus: map[string]NodeSecretStatus{
					"node1": {AllSecretsUpdated: true}, // Has current but not expected
					"node2": {AllSecretsUpdated: true}, // Has current but not expected
				},
			},
			totalNodes:         2,
			expectedUpdated:    0, // Should be 0 when drift detected
			expectedAllUpdated: false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			summary := computeDeploymentSummary(tt.trackingData, tt.totalNodes, "test-configmap")

			if summary.UpdatedNodes != tt.expectedUpdated {
				t.Errorf("computeDeploymentSummary() UpdatedNodes = %d, want %d",
					summary.UpdatedNodes, tt.expectedUpdated)
			}

			if summary.AllNodesUpdated != tt.expectedAllUpdated {
				t.Errorf("computeDeploymentSummary() AllNodesUpdated = %v, want %v",
					summary.AllNodesUpdated, tt.expectedAllUpdated)
			}

			if summary.TotalNodes != tt.totalNodes {
				t.Errorf("computeDeploymentSummary() TotalNodes = %d, want %d",
					summary.TotalNodes, tt.totalNodes)
			}

			if summary.ConfigMapName != "test-configmap" {
				t.Errorf("computeDeploymentSummary() ConfigMapName = %s, want test-configmap",
					summary.ConfigMapName)
			}

			if summary.LastUpdateTime == nil {
				t.Error("computeDeploymentSummary() LastUpdateTime is nil, want timestamp")
			}
		})
	}
}

func TestSecretTrackingDataJSONSerialization(t *testing.T) {
	now := time.Now()
	original := &SecretTrackingData{
		Secrets: map[string]SecretVersionInfo{
			"secret1": {
				CurrentHash:       "hash123",
				PreviousHash:      "hash456",
				NodesWithCurrent:  []string{"node1", "node2"},
				NodesWithPrevious: []string{"node3"},
				LastChanged:       now,
			},
			"secret2": {
				CurrentHash:      "hash789",
				NodesWithCurrent: []string{"node1", "node2", "node3"},
				LastChanged:      now,
			},
		},
		NodeStatus: map[string]NodeSecretStatus{
			"node1": {
				AllSecretsUpdated:  true,
				SecretsWithCurrent: []string{"secret1", "secret2"},
			},
			"node2": {
				AllSecretsUpdated:  true,
				SecretsWithCurrent: []string{"secret1", "secret2"},
			},
			"node3": {
				AllSecretsUpdated:   false,
				SecretsWithCurrent:  []string{"secret2"},
				SecretsWithPrevious: []string{"secret1"},
			},
		},
	}

	// Marshal to JSON
	jsonData, err := json.Marshal(original)
	if err != nil {
		t.Fatalf("Failed to marshal SecretTrackingData: %v", err)
	}

	// Unmarshal back
	var restored SecretTrackingData
	if err := json.Unmarshal(jsonData, &restored); err != nil {
		t.Fatalf("Failed to unmarshal SecretTrackingData: %v", err)
	}

	// Verify secrets
	if len(restored.Secrets) != len(original.Secrets) {
		t.Errorf("Restored secrets count = %d, want %d", len(restored.Secrets), len(original.Secrets))
	}

	secret1 := restored.Secrets["secret1"]
	if secret1.CurrentHash != "hash123" {
		t.Errorf("Restored secret1 CurrentHash = %s, want hash123", secret1.CurrentHash)
	}
	if secret1.PreviousHash != "hash456" {
		t.Errorf("Restored secret1 PreviousHash = %s, want hash456", secret1.PreviousHash)
	}
	if len(secret1.NodesWithCurrent) != 2 {
		t.Errorf("Restored secret1 NodesWithCurrent count = %d, want 2", len(secret1.NodesWithCurrent))
	}

	// Verify node status
	if len(restored.NodeStatus) != len(original.NodeStatus) {
		t.Errorf("Restored node status count = %d, want %d", len(restored.NodeStatus), len(original.NodeStatus))
	}

	node1 := restored.NodeStatus["node1"]
	if !node1.AllSecretsUpdated {
		t.Error("Restored node1 AllSecretsUpdated = false, want true")
	}
	if len(node1.SecretsWithCurrent) != 2 {
		t.Errorf("Restored node1 SecretsWithCurrent count = %d, want 2", len(node1.SecretsWithCurrent))
	}
}

func TestGetSecretTrackingConfigMapName(t *testing.T) {
	tests := []struct {
		nodesetName string
		expected    string
	}{
		{
			nodesetName: "compute-nodes",
			expected:    "compute-nodes-secret-tracking",
		},
		{
			nodesetName: "edpm",
			expected:    "edpm-secret-tracking",
		},
		{
			nodesetName: "my-nodeset-123",
			expected:    "my-nodeset-123-secret-tracking",
		},
	}

	for _, tt := range tests {
		t.Run(tt.nodesetName, func(t *testing.T) {
			result := getSecretTrackingConfigMapName(tt.nodesetName)
			if result != tt.expected {
				t.Errorf("getSecretTrackingConfigMapName(%q) = %q, want %q",
					tt.nodesetName, result, tt.expected)
			}
		})
	}
}

func TestSecretRotationTracking(t *testing.T) {
	// Simulate secret rotation scenario
	trackingData := &SecretTrackingData{
		Secrets:    make(map[string]SecretVersionInfo),
		NodeStatus: make(map[string]NodeSecretStatus),
	}

	now := time.Now()

	// Step 1: Initial secret deployment
	trackingData.Secrets["rabbitmq-secret"] = SecretVersionInfo{
		CurrentHash:      "hash-v1",
		NodesWithCurrent: []string{"node1", "node2"},
		LastChanged:      now,
	}

	// Verify initial state
	secret := trackingData.Secrets["rabbitmq-secret"]
	if secret.CurrentHash != "hash-v1" {
		t.Errorf("Initial CurrentHash = %s, want hash-v1", secret.CurrentHash)
	}
	if secret.PreviousHash != "" {
		t.Errorf("Initial PreviousHash = %s, want empty", secret.PreviousHash)
	}

	// Step 2: Simulate rotation - hash changes
	secret.PreviousHash = secret.CurrentHash
	secret.NodesWithPrevious = secret.NodesWithCurrent
	secret.CurrentHash = "hash-v2"
	secret.NodesWithCurrent = []string{"node1"} // Only node1 has new version
	secret.LastChanged = now.Add(1 * time.Hour)
	trackingData.Secrets["rabbitmq-secret"] = secret

	// Verify rotation state
	secret = trackingData.Secrets["rabbitmq-secret"]
	if secret.CurrentHash != "hash-v2" {
		t.Errorf("After rotation CurrentHash = %s, want hash-v2", secret.CurrentHash)
	}
	if secret.PreviousHash != "hash-v1" {
		t.Errorf("After rotation PreviousHash = %s, want hash-v1", secret.PreviousHash)
	}
	if len(secret.NodesWithPrevious) != 2 {
		t.Errorf("After rotation NodesWithPrevious count = %d, want 2", len(secret.NodesWithPrevious))
	}
	if len(secret.NodesWithCurrent) != 1 {
		t.Errorf("After rotation NodesWithCurrent count = %d, want 1", len(secret.NodesWithCurrent))
	}

	// Step 3: Second node gets new version
	secret.NodesWithCurrent = append(secret.NodesWithCurrent, "node2")
	trackingData.Secrets["rabbitmq-secret"] = secret

	// All nodes now have current version, should clear previous
	if len(secret.NodesWithCurrent) == 2 {
		secret.PreviousHash = ""
		secret.NodesWithPrevious = []string{}
		trackingData.Secrets["rabbitmq-secret"] = secret
	}

	// Verify cleanup after full rotation
	secret = trackingData.Secrets["rabbitmq-secret"]
	if secret.PreviousHash != "" {
		t.Errorf("After full rotation PreviousHash = %s, want empty", secret.PreviousHash)
	}
	if len(secret.NodesWithPrevious) != 0 {
		t.Errorf("After full rotation NodesWithPrevious count = %d, want 0", len(secret.NodesWithPrevious))
	}
	if len(secret.NodesWithCurrent) != 2 {
		t.Errorf("After full rotation NodesWithCurrent count = %d, want 2", len(secret.NodesWithCurrent))
	}
}

func TestNodeAccumulationDoesNotDuplicateInBothLists(t *testing.T) {
	// This test verifies that when nodes accumulate across deployments,
	// they are removed from nodesWithPrevious when added to nodesWithCurrent

	// Initial state: secret rotated, compute-0 has new version, compute-1 has old
	trackingData := &SecretTrackingData{
		Secrets: map[string]SecretVersionInfo{
			"nova-config": {
				CurrentHash:       "hash-v2",
				PreviousHash:      "hash-v1",
				NodesWithCurrent:  []string{"compute-0"},
				NodesWithPrevious: []string{"compute-1"},
				LastChanged:       time.Now(),
			},
		},
		NodeStatus: map[string]NodeSecretStatus{
			"compute-0": {AllSecretsUpdated: true, SecretsWithCurrent: []string{"nova-config"}},
			"compute-1": {AllSecretsUpdated: false, SecretsWithPrevious: []string{"nova-config"}},
		},
	}

	// Simulate deployment that covers compute-1 with hash-v2 (same version accumulation)
	secretInfo := trackingData.Secrets["nova-config"]
	coveredNodes := []string{"compute-1"}

	// Add newly covered nodes (this is the code path that had the bug)
	for _, node := range coveredNodes {
		if !slices.Contains(secretInfo.NodesWithCurrent, node) {
			secretInfo.NodesWithCurrent = append(secretInfo.NodesWithCurrent, node)
		}

		// Remove from previous if it was there (the fix)
		if secretInfo.PreviousHash != "" {
			newPrevious := []string{}
			for _, prevNode := range secretInfo.NodesWithPrevious {
				if prevNode != node {
					newPrevious = append(newPrevious, prevNode)
				}
			}
			secretInfo.NodesWithPrevious = newPrevious
		}
	}

	trackingData.Secrets["nova-config"] = secretInfo

	// Verify compute-1 is only in nodesWithCurrent, not in both
	if !slices.Contains(secretInfo.NodesWithCurrent, "compute-1") {
		t.Error("compute-1 should be in nodesWithCurrent after deployment")
	}

	if slices.Contains(secretInfo.NodesWithPrevious, "compute-1") {
		t.Error("compute-1 should NOT be in nodesWithPrevious after being upgraded")
	}

	// Verify both nodes are in current
	if len(secretInfo.NodesWithCurrent) != 2 {
		t.Errorf("Expected 2 nodes in nodesWithCurrent, got %d", len(secretInfo.NodesWithCurrent))
	}

	if len(secretInfo.NodesWithPrevious) != 0 {
		t.Errorf("Expected 0 nodes in nodesWithPrevious, got %d", len(secretInfo.NodesWithPrevious))
	}
}

func TestGradualRolloutWithAnsibleLimit(t *testing.T) {
	// Test gradual rollout: deployment 1 covers compute-0, deployment 2 covers compute-1
	tests := []struct {
		name                 string
		initialTracking      *SecretTrackingData
		deployment1Nodes     []string // First deployment with AnsibleLimit
		deployment2Nodes     []string // Second deployment with AnsibleLimit
		expectedAllUpdated   bool
		expectedUpdatedCount int
	}{
		{
			name: "gradual rollout - first deployment partial",
			initialTracking: &SecretTrackingData{
				Secrets:    make(map[string]SecretVersionInfo),
				NodeStatus: make(map[string]NodeSecretStatus),
			},
			deployment1Nodes:     []string{"compute-0"},
			deployment2Nodes:     []string{"compute-1"},
			expectedAllUpdated:   true,
			expectedUpdatedCount: 2,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tracking := tt.initialTracking

			// Deployment 1: covers some nodes
			secretInfo := SecretVersionInfo{
				CurrentHash:      "hash1",
				ExpectedHash:     "hash1", // No drift - cluster matches what's deployed
				NodesWithCurrent: tt.deployment1Nodes,
				LastChanged:      time.Now(),
			}
			tracking.Secrets["test-secret"] = secretInfo

			// Deployment 2: accumulates more nodes
			for _, node := range tt.deployment2Nodes {
				if !slices.Contains(secretInfo.NodesWithCurrent, node) {
					secretInfo.NodesWithCurrent = append(secretInfo.NodesWithCurrent, node)
				}
			}
			tracking.Secrets["test-secret"] = secretInfo

			// Compute node status
			allNodes := []string{"compute-0", "compute-1"}
			for _, nodeName := range allNodes {
				nodeStatus := NodeSecretStatus{
					AllSecretsUpdated:   true,
					SecretsWithCurrent:  []string{},
					SecretsWithPrevious: []string{},
				}

				for secretName, sInfo := range tracking.Secrets {
					if slices.Contains(sInfo.NodesWithCurrent, nodeName) {
						nodeStatus.SecretsWithCurrent = append(nodeStatus.SecretsWithCurrent, secretName)
					} else {
						nodeStatus.AllSecretsUpdated = false
					}
				}

				tracking.NodeStatus[nodeName] = nodeStatus
			}

			// Compute summary
			summary := computeDeploymentSummary(tracking, len(allNodes), "test-cm")

			if summary.AllNodesUpdated != tt.expectedAllUpdated {
				t.Errorf("AllNodesUpdated = %v, want %v", summary.AllNodesUpdated, tt.expectedAllUpdated)
			}

			if summary.UpdatedNodes != tt.expectedUpdatedCount {
				t.Errorf("UpdatedNodes = %d, want %d", summary.UpdatedNodes, tt.expectedUpdatedCount)
			}
		})
	}
}

func TestSecretRotationWithGradualRollout(t *testing.T) {
	// Test secret rotation followed by gradual rollout
	tests := []struct {
		name                 string
		phase                string
		expectedNodesInBoth  bool // Should any node be in both current and previous?
		expectedAllUpdated   bool
		expectedUpdatedNodes int
	}{
		{
			name:                 "after rotation - one node upgraded",
			phase:                "partial",
			expectedNodesInBoth:  false,
			expectedAllUpdated:   false,
			expectedUpdatedNodes: 1,
		},
		{
			name:                 "after rotation - all nodes upgraded",
			phase:                "complete",
			expectedNodesInBoth:  false,
			expectedAllUpdated:   true,
			expectedUpdatedNodes: 2,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Initial: all nodes have hash-v1 (Current == Expected, no drift)
			tracking := &SecretTrackingData{
				Secrets: map[string]SecretVersionInfo{
					"nova-config": {
						CurrentHash:      "hash-v1",
						ExpectedHash:     "hash-v1",
						NodesWithCurrent: []string{"compute-0", "compute-1"},
						LastChanged:      time.Now(),
					},
				},
				NodeStatus: make(map[string]NodeSecretStatus),
			}

			// Rotation happens: hash changes to v2
			secretInfo := tracking.Secrets["nova-config"]
			secretInfo.PreviousHash = secretInfo.CurrentHash
			secretInfo.NodesWithPrevious = secretInfo.NodesWithCurrent
			secretInfo.CurrentHash = "hash-v2"
			secretInfo.ExpectedHash = "hash-v2"      // Expected also updated
			secretInfo.NodesWithCurrent = []string{} // Reset
			tracking.Secrets["nova-config"] = secretInfo

			// Deployment 1: covers compute-0 with hash-v2
			secretInfo.NodesWithCurrent = append(secretInfo.NodesWithCurrent, "compute-0")

			// Remove from previous (the fix)
			newPrevious := []string{}
			for _, node := range secretInfo.NodesWithPrevious {
				if node != "compute-0" {
					newPrevious = append(newPrevious, node)
				}
			}
			secretInfo.NodesWithPrevious = newPrevious
			tracking.Secrets["nova-config"] = secretInfo

			if tt.phase == "complete" {
				// Deployment 2: covers compute-1 with hash-v2
				secretInfo.NodesWithCurrent = append(secretInfo.NodesWithCurrent, "compute-1")

				// Remove from previous
				secretInfo.NodesWithPrevious = []string{} // Should be empty now
				secretInfo.PreviousHash = ""              // Clear metadata
				tracking.Secrets["nova-config"] = secretInfo
			}

			// Verify no node is in both lists
			secretInfo = tracking.Secrets["nova-config"]
			for _, node := range secretInfo.NodesWithCurrent {
				if slices.Contains(secretInfo.NodesWithPrevious, node) {
					if !tt.expectedNodesInBoth {
						t.Errorf("Node %q is in BOTH nodesWithCurrent and nodesWithPrevious", node)
					}
				}
			}

			// Compute node status
			allNodes := []string{"compute-0", "compute-1"}
			for _, nodeName := range allNodes {
				nodeStatus := NodeSecretStatus{
					AllSecretsUpdated:   true,
					SecretsWithCurrent:  []string{},
					SecretsWithPrevious: []string{},
				}

				for secretName, sInfo := range tracking.Secrets {
					if slices.Contains(sInfo.NodesWithCurrent, nodeName) {
						nodeStatus.SecretsWithCurrent = append(nodeStatus.SecretsWithCurrent, secretName)
					} else if slices.Contains(sInfo.NodesWithPrevious, nodeName) {
						nodeStatus.SecretsWithPrevious = append(nodeStatus.SecretsWithPrevious, secretName)
						nodeStatus.AllSecretsUpdated = false
					} else {
						nodeStatus.AllSecretsUpdated = false
					}
				}

				tracking.NodeStatus[nodeName] = nodeStatus
			}

			// Compute summary
			summary := computeDeploymentSummary(tracking, len(allNodes), "test-cm")

			if summary.AllNodesUpdated != tt.expectedAllUpdated {
				t.Errorf("AllNodesUpdated = %v, want %v", summary.AllNodesUpdated, tt.expectedAllUpdated)
			}

			if summary.UpdatedNodes != tt.expectedUpdatedNodes {
				t.Errorf("UpdatedNodes = %d, want %d", summary.UpdatedNodes, tt.expectedUpdatedNodes)
			}
		})
	}
}

func TestRotationImmediatelyCoveringAllNodes(t *testing.T) {
	// Test the bug where rotation+full coverage left nodes in both lists
	// This was the actual bug seen in production

	tracking := &SecretTrackingData{
		Secrets: map[string]SecretVersionInfo{
			"nova-config": {
				CurrentHash:      "hash-v1",
				NodesWithCurrent: []string{"compute-0", "compute-1"},
				LastChanged:      time.Now(),
			},
		},
		NodeStatus: make(map[string]NodeSecretStatus),
	}

	// Rotation happens: hash changes to v2
	secretInfo := tracking.Secrets["nova-config"]

	// Move current to previous (rotation logic)
	secretInfo.PreviousHash = secretInfo.CurrentHash
	secretInfo.NodesWithPrevious = secretInfo.NodesWithCurrent

	// Update to new version
	secretInfo.CurrentHash = "hash-v2"

	// Deployment immediately covers all nodes
	coveredNodes := []string{"compute-0", "compute-1"}
	secretInfo.NodesWithCurrent = coveredNodes

	totalNodes := 2

	// Clear previous if all nodes covered (the fix)
	if len(secretInfo.NodesWithCurrent) == totalNodes && secretInfo.PreviousHash != "" {
		secretInfo.PreviousHash = ""
		secretInfo.NodesWithPrevious = []string{}
	}

	// Set Expected to match Current (no drift after deployment completes)
	secretInfo.ExpectedHash = secretInfo.CurrentHash

	tracking.Secrets["nova-config"] = secretInfo

	// Verify nodes NOT in both lists
	if len(secretInfo.NodesWithPrevious) != 0 {
		t.Errorf("After rotation with full coverage, nodesWithPrevious should be empty, got %v",
			secretInfo.NodesWithPrevious)
	}

	if len(secretInfo.NodesWithCurrent) != 2 {
		t.Errorf("After rotation with full coverage, nodesWithCurrent should have 2 nodes, got %d",
			len(secretInfo.NodesWithCurrent))
	}

	if secretInfo.PreviousHash != "" {
		t.Errorf("After rotation with full coverage, previousHash should be cleared, got %q",
			secretInfo.PreviousHash)
	}

	// Compute node status
	allNodes := []string{"compute-0", "compute-1"}
	for _, nodeName := range allNodes {
		nodeStatus := NodeSecretStatus{
			AllSecretsUpdated:   true,
			SecretsWithCurrent:  []string{},
			SecretsWithPrevious: []string{},
		}

		for secretName, sInfo := range tracking.Secrets {
			if slices.Contains(sInfo.NodesWithCurrent, nodeName) {
				nodeStatus.SecretsWithCurrent = append(nodeStatus.SecretsWithCurrent, secretName)
			} else if slices.Contains(sInfo.NodesWithPrevious, nodeName) {
				nodeStatus.SecretsWithPrevious = append(nodeStatus.SecretsWithPrevious, secretName)
				nodeStatus.AllSecretsUpdated = false
			} else {
				nodeStatus.AllSecretsUpdated = false
			}
		}

		tracking.NodeStatus[nodeName] = nodeStatus
	}

	// Both nodes should be fully updated
	for _, nodeName := range allNodes {
		if !tracking.NodeStatus[nodeName].AllSecretsUpdated {
			t.Errorf("Node %q should have AllSecretsUpdated=true after rotation with full coverage", nodeName)
		}
	}

	// Summary should show all updated
	summary := computeDeploymentSummary(tracking, totalNodes, "test-cm")
	if !summary.AllNodesUpdated {
		t.Error("AllNodesUpdated should be true after rotation with full coverage")
	}
	if summary.UpdatedNodes != 2 {
		t.Errorf("UpdatedNodes should be 2 after rotation with full coverage, got %d", summary.UpdatedNodes)
	}
}

func TestMultipleSecretsWithDifferentRolloutStates(t *testing.T) {
	// Test scenario with multiple secrets at different rollout stages
	tracking := &SecretTrackingData{
		Secrets: map[string]SecretVersionInfo{
			"secret-a": {
				CurrentHash:      "hash-a1",
				ExpectedHash:     "hash-a1", // No drift
				NodesWithCurrent: []string{"compute-0", "compute-1"},
				LastChanged:      time.Now(),
			},
			"secret-b": {
				CurrentHash:       "hash-b2",
				ExpectedHash:      "hash-b2", // No drift
				PreviousHash:      "hash-b1",
				NodesWithCurrent:  []string{"compute-0"},
				NodesWithPrevious: []string{"compute-1"},
				LastChanged:       time.Now(),
			},
			"secret-c": {
				CurrentHash:      "hash-c1",
				ExpectedHash:     "hash-c1", // No drift
				NodesWithCurrent: []string{"compute-0"},
				LastChanged:      time.Now(),
			},
		},
		NodeStatus: make(map[string]NodeSecretStatus),
	}

	// Compute node status
	allNodes := []string{"compute-0", "compute-1"}
	for _, nodeName := range allNodes {
		nodeStatus := NodeSecretStatus{
			AllSecretsUpdated:   true,
			SecretsWithCurrent:  []string{},
			SecretsWithPrevious: []string{},
		}

		for secretName, secretInfo := range tracking.Secrets {
			if slices.Contains(secretInfo.NodesWithCurrent, nodeName) {
				nodeStatus.SecretsWithCurrent = append(nodeStatus.SecretsWithCurrent, secretName)
			} else if slices.Contains(secretInfo.NodesWithPrevious, nodeName) {
				nodeStatus.SecretsWithPrevious = append(nodeStatus.SecretsWithPrevious, secretName)
				nodeStatus.AllSecretsUpdated = false
			} else {
				nodeStatus.AllSecretsUpdated = false
			}
		}

		tracking.NodeStatus[nodeName] = nodeStatus
	}

	// Compute summary
	summary := computeDeploymentSummary(tracking, len(allNodes), "test-cm")

	// compute-0 has all current versions (a, b, c)
	if !tracking.NodeStatus["compute-0"].AllSecretsUpdated {
		t.Error("compute-0 should have all secrets updated")
	}

	// compute-1 missing secret-c and has old version of secret-b
	if tracking.NodeStatus["compute-1"].AllSecretsUpdated {
		t.Error("compute-1 should NOT have all secrets updated")
	}

	// Only 1 node fully updated
	if summary.UpdatedNodes != 1 {
		t.Errorf("UpdatedNodes = %d, want 1", summary.UpdatedNodes)
	}

	if summary.AllNodesUpdated {
		t.Error("AllNodesUpdated should be false")
	}
}

func TestDetectSecretDrift(t *testing.T) {
	now := time.Now()

	tests := []struct {
		name         string
		trackingData *SecretTrackingData
		secrets      []*corev1.Secret
		wantDrift    bool
		wantErr      bool
	}{
		{
			name:         "no tracking data",
			trackingData: nil,
			wantDrift:    true, // fail-safe: no tracking = assume drift
			wantErr:      false,
		},
		{
			name: "no drift - ResourceVersion and Generation match",
			trackingData: &SecretTrackingData{
				Secrets: map[string]SecretVersionInfo{
					"rabbitmq-secret": {
						CurrentHash:            "abc123",
						CurrentResourceVersion: "v1",
						CurrentGeneration:      1,
						NodesWithCurrent:       []string{"node1", "node2"},
						LastChanged:            now,
					},
				},
			},
			secrets: []*corev1.Secret{
				{
					ObjectMeta: metav1.ObjectMeta{
						Name:            "rabbitmq-secret",
						Namespace:       "test",
						ResourceVersion: "v1",
						Generation:      1,
					},
					Data: map[string][]byte{
						"username": []byte("user"),
						"password": []byte("pass"),
					},
				},
			},
			wantDrift: false,
			wantErr:   false,
		},
		{
			name: "drift detected - ResourceVersion changed",
			trackingData: &SecretTrackingData{
				Secrets: map[string]SecretVersionInfo{
					"rabbitmq-secret": {
						CurrentHash:            "old-hash",
						CurrentResourceVersion: "v1",
						CurrentGeneration:      1,
						NodesWithCurrent:       []string{"node1", "node2"},
						LastChanged:            now,
					},
				},
			},
			secrets: []*corev1.Secret{
				{
					ObjectMeta: metav1.ObjectMeta{
						Name:            "rabbitmq-secret",
						Namespace:       "test",
						ResourceVersion: "v2", // Changed
						Generation:      1,
					},
					Data: map[string][]byte{
						"username": []byte("newuser"),
						"password": []byte("newpass"),
					},
				},
			},
			wantDrift: true,
			wantErr:   false,
		},
		{
			name: "drift detected - secret deleted",
			trackingData: &SecretTrackingData{
				Secrets: map[string]SecretVersionInfo{
					"rabbitmq-secret": {
						CurrentHash:            "abc123",
						CurrentResourceVersion: "v1",
						CurrentGeneration:      1,
						NodesWithCurrent:       []string{"node1"},
						LastChanged:            now,
					},
				},
			},
			secrets:   []*corev1.Secret{}, // Secret doesn't exist
			wantDrift: true,
			wantErr:   false,
		},
		{
			name: "drift detected - Generation changed",
			trackingData: &SecretTrackingData{
				Secrets: map[string]SecretVersionInfo{
					"rabbitmq-secret": {
						CurrentHash:            "abc123",
						CurrentResourceVersion: "v1",
						CurrentGeneration:      1,
						NodesWithCurrent:       []string{"node1"},
						LastChanged:            now,
					},
				},
			},
			secrets: []*corev1.Secret{
				{
					ObjectMeta: metav1.ObjectMeta{
						Name:            "rabbitmq-secret",
						Namespace:       "test",
						ResourceVersion: "v1",
						Generation:      2, // Changed
					},
					Data: map[string][]byte{
						"username": []byte("user"),
						"password": []byte("pass"),
					},
				},
			},
			wantDrift: true,
			wantErr:   false,
		},
		{
			name: "multiple secrets - one drifted",
			trackingData: &SecretTrackingData{
				Secrets: map[string]SecretVersionInfo{
					"secret1": {
						CurrentHash:            "hash1",
						CurrentResourceVersion: "v1",
						CurrentGeneration:      1,
						NodesWithCurrent:       []string{"node1"},
						LastChanged:            now,
					},
					"secret2": {
						CurrentHash:            "old-hash",
						CurrentResourceVersion: "v1",
						CurrentGeneration:      1,
						NodesWithCurrent:       []string{"node1"},
						LastChanged:            now,
					},
				},
			},
			secrets: []*corev1.Secret{
				{
					ObjectMeta: metav1.ObjectMeta{
						Name:            "secret1",
						Namespace:       "test",
						ResourceVersion: "v1",
						Generation:      1,
					},
					Data: map[string][]byte{"key": []byte("value1")},
				},
				{
					ObjectMeta: metav1.ObjectMeta{
						Name:            "secret2",
						Namespace:       "test",
						ResourceVersion: "v2", // Changed
						Generation:      1,
					},
					Data: map[string][]byte{"key": []byte("newvalue")},
				},
			},
			wantDrift: true,
			wantErr:   false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Setup fake client
			scheme := runtime.NewScheme()
			_ = corev1.AddToScheme(scheme)
			_ = dataplanev1.AddToScheme(scheme)

			objs := make([]runtime.Object, len(tt.secrets))
			for i, s := range tt.secrets {
				objs[i] = s
			}

			fakeClient := fake.NewClientBuilder().
				WithScheme(scheme).
				WithRuntimeObjects(objs...).
				Build()

			reconciler := &OpenStackDataPlaneNodeSetReconciler{
				Client: fakeClient,
			}

			instance := &dataplanev1.OpenStackDataPlaneNodeSet{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "test-nodeset",
					Namespace: "test",
				},
			}

			// For no-drift cases, update tracking data with actual secret metadata
			// Set Current = Expected = actual secret values
			if tt.trackingData != nil && !tt.wantDrift && len(tt.secrets) > 0 {
				for secretName, secretInfo := range tt.trackingData.Secrets {
					for _, sec := range tt.secrets {
						if sec.Name == secretName && sec.Data != nil {
							// Compute hash using secret.Hash from lib-common (deterministic)
							hash, _ := secret.Hash(sec)

							// Set Current and Expected to match (no drift)
							secretInfo.CurrentHash = hash
							secretInfo.CurrentResourceVersion = sec.ResourceVersion
							secretInfo.CurrentGeneration = sec.Generation
							secretInfo.ExpectedHash = hash
							secretInfo.ExpectedResourceVersion = sec.ResourceVersion
							secretInfo.ExpectedGeneration = sec.Generation
							tt.trackingData.Secrets[secretName] = secretInfo
						}
					}
				}
			}

			drift, err := reconciler.detectSecretDrift(context.Background(), instance, tt.trackingData)

			if (err != nil) != tt.wantErr {
				t.Errorf("detectSecretDrift() error = %v, wantErr %v", err, tt.wantErr)
				return
			}

			if drift != tt.wantDrift {
				t.Errorf("detectSecretDrift() drift = %v, want %v", drift, tt.wantDrift)
			}
		})
	}
}

// TestDeploymentAfterRotationUpdatesTracking verifies that when a deployment
// completes after a secret rotation, the tracking is updated regardless of
// when the deployment was created relative to when we detected the rotation.
func TestDeploymentAfterRotationUpdatesTracking(t *testing.T) {
	// Scenario:
	// - Secret at V1 initially
	// - Deployment created and runs
	// - Secret changes to V2 during deployment
	// - Deployment completes with V2
	// - We process it and detect rotation
	// - Should update Current=V2, NodesWithCurrent=[all nodes]
	//
	// The key test: we don't check deployment creation time vs LastChanged.
	// We trust that if deployment is ready, nodes have what deployment deployed.

	trackingData := &SecretTrackingData{
		Secrets: map[string]SecretVersionInfo{
			"nova-config": {
				CurrentHash:             "hash-v1",
				CurrentResourceVersion:  "100",
				CurrentGeneration:       1,
				ExpectedHash:            "hash-v1",
				ExpectedResourceVersion: "100",
				ExpectedGeneration:      1,
				NodesWithCurrent:        []string{"compute-0", "compute-1"},
				LastChanged:             time.Now().Add(-1 * time.Hour),
			},
		},
		NodeStatus: make(map[string]NodeSecretStatus),
	}

	// Simulate processing a deployment that has V2 (ResourceVersion 200)
	// The code detects rotation because 200 != 100
	secretInfo := trackingData.Secrets["nova-config"]

	// This simulates what the code does when rotation is detected:
	// Move Current to Previous
	secretInfo.PreviousHash = secretInfo.CurrentHash
	secretInfo.PreviousResourceVersion = secretInfo.CurrentResourceVersion
	secretInfo.PreviousGeneration = secretInfo.CurrentGeneration
	secretInfo.NodesWithPrevious = secretInfo.NodesWithCurrent

	// Update Current and Expected to new version
	secretInfo.CurrentHash = "hash-v2"
	secretInfo.CurrentResourceVersion = "200"
	secretInfo.CurrentGeneration = 1
	secretInfo.ExpectedHash = "hash-v2"
	secretInfo.ExpectedResourceVersion = "200"
	secretInfo.ExpectedGeneration = 1
	secretInfo.LastChanged = time.Now()

	// If deployment is ready, update nodes (removed the timing check)
	isDeploymentReady := true
	coveredNodes := []string{"compute-0", "compute-1"}

	if isDeploymentReady {
		secretInfo.NodesWithCurrent = coveredNodes

		// Clear previous if all nodes updated
		totalNodes := 2
		if len(secretInfo.NodesWithCurrent) == totalNodes && secretInfo.PreviousHash != "" {
			secretInfo.PreviousHash = ""
			secretInfo.PreviousResourceVersion = ""
			secretInfo.PreviousGeneration = 0
			secretInfo.NodesWithPrevious = []string{}
		}
	}

	trackingData.Secrets["nova-config"] = secretInfo

	// Update per-node status (this is what the actual code does)
	allNodes := []string{"compute-0", "compute-1"}
	for _, nodeName := range allNodes {
		nodeStatus := NodeSecretStatus{
			AllSecretsUpdated:   true,
			SecretsWithCurrent:  []string{},
			SecretsWithPrevious: []string{},
		}

		// Check which secrets this node has
		for secretName, secretInfo := range trackingData.Secrets {
			if slices.Contains(secretInfo.NodesWithCurrent, nodeName) {
				nodeStatus.SecretsWithCurrent = append(nodeStatus.SecretsWithCurrent, secretName)
			} else if slices.Contains(secretInfo.NodesWithPrevious, nodeName) {
				nodeStatus.SecretsWithPrevious = append(nodeStatus.SecretsWithPrevious, secretName)
				nodeStatus.AllSecretsUpdated = false
			} else {
				// Node doesn't have this secret at all
				nodeStatus.AllSecretsUpdated = false
			}
		}

		trackingData.NodeStatus[nodeName] = nodeStatus
	}

	// Verify the tracking was updated correctly
	if secretInfo.CurrentResourceVersion != "200" {
		t.Errorf("CurrentResourceVersion = %s, want 200", secretInfo.CurrentResourceVersion)
	}

	if len(secretInfo.NodesWithCurrent) != 2 {
		t.Errorf("NodesWithCurrent count = %d, want 2", len(secretInfo.NodesWithCurrent))
	}

	if !slices.Contains(secretInfo.NodesWithCurrent, "compute-0") {
		t.Error("compute-0 should be in NodesWithCurrent")
	}

	if !slices.Contains(secretInfo.NodesWithCurrent, "compute-1") {
		t.Error("compute-1 should be in NodesWithCurrent")
	}

	// Previous should be cleared since all nodes have current
	if secretInfo.PreviousHash != "" {
		t.Errorf("PreviousHash should be empty, got %s", secretInfo.PreviousHash)
	}

	if len(secretInfo.NodesWithPrevious) != 0 {
		t.Errorf("NodesWithPrevious count = %d, want 0", len(secretInfo.NodesWithPrevious))
	}

	// Verify summary calculation shows all updated
	summary := computeDeploymentSummary(trackingData, 2, "test-configmap")

	if !summary.AllNodesUpdated {
		t.Error("AllNodesUpdated should be true when all nodes have current version")
	}

	if summary.UpdatedNodes != 2 {
		t.Errorf("UpdatedNodes = %d, want 2", summary.UpdatedNodes)
	}
}

// TestDriftDetectionBeforeDeploymentProcessing tests the scenario where:
// 1. Secret rotates in cluster
// 2. Drift detection runs and updates Expected to new version
// 3. Then deployment completes with new version
// 4. Deployment processing should still update Current correctly
// This was the bug reported in /tmp/tracking-fix-analysis.md
func TestDriftDetectionBeforeDeploymentProcessing(t *testing.T) {
	// Initial state: nodes have secret V1
	trackingData := &SecretTrackingData{
		Secrets: map[string]SecretVersionInfo{
			"nova-config": {
				CurrentHash:             "hash-v1",
				CurrentResourceVersion:  "100",
				CurrentGeneration:       1,
				ExpectedHash:            "hash-v1",
				ExpectedResourceVersion: "100",
				ExpectedGeneration:      1,
				NodesWithCurrent:        []string{"compute-0", "compute-1"},
			},
		},
		NodeStatus: map[string]NodeSecretStatus{},
	}

	// STEP 1: Secret rotates in cluster to V2
	// (happens outside our control - user creates new RabbitMQUser, secret changes)

	// STEP 2: Drift detection runs and detects the change
	// It updates Expected to match cluster (V2)
	secretInfo := trackingData.Secrets["nova-config"]
	secretInfo.ExpectedHash = "hash-v2"
	secretInfo.ExpectedResourceVersion = "200"
	secretInfo.ExpectedGeneration = 2
	trackingData.Secrets["nova-config"] = secretInfo

	// At this point: Current=V1, Expected=V2 → drift detected
	if secretInfo.CurrentResourceVersion == secretInfo.ExpectedResourceVersion {
		t.Error("Drift detection should show Current != Expected")
	}

	// STEP 3: User creates deployment, it completes with V2
	// deployment.Status.SecretHashes has hash-v2 (from deployment controller)
	deploymentSecretHash := "hash-v2"

	// STEP 4: Deployment processing runs
	// Key test: should it detect rotation even though Expected already = V2?
	// Old bug: compared Expected (V2) vs cluster (V2) → no rotation detected
	// New fix: compares Current (V1) vs deployment hash (V2) → rotation detected!

	secretInfo = trackingData.Secrets["nova-config"]

	// This is the key comparison - using deployment hash, not cluster ResourceVersion
	if secretInfo.CurrentHash != deploymentSecretHash {
		// ROTATION DETECTED! (correct behavior)
		// Move Current → Previous
		secretInfo.PreviousHash = secretInfo.CurrentHash
		secretInfo.PreviousResourceVersion = secretInfo.CurrentResourceVersion
		secretInfo.PreviousGeneration = secretInfo.CurrentGeneration
		secretInfo.NodesWithPrevious = secretInfo.NodesWithCurrent

		// Update Current to deployment version
		secretInfo.CurrentHash = deploymentSecretHash
		secretInfo.CurrentResourceVersion = "200" // from cluster fetch
		secretInfo.CurrentGeneration = 2

		// Deployment is ready, all nodes covered
		secretInfo.NodesWithCurrent = []string{"compute-0", "compute-1"}

		// All nodes updated → clear previous
		if len(secretInfo.NodesWithCurrent) == 2 {
			secretInfo.PreviousHash = ""
			secretInfo.PreviousResourceVersion = ""
			secretInfo.PreviousGeneration = 0
			secretInfo.NodesWithPrevious = []string{}
		}

		trackingData.Secrets["nova-config"] = secretInfo
	} else {
		t.Fatal("Rotation should be detected even after drift detection updated Expected")
	}

	// Update per-node status (normally done by deployment processing)
	for _, nodeName := range []string{"compute-0", "compute-1"} {
		nodeStatus := NodeSecretStatus{
			AllSecretsUpdated:   true,
			SecretsWithCurrent:  []string{"nova-config"},
			SecretsWithPrevious: []string{},
		}
		trackingData.NodeStatus[nodeName] = nodeStatus
	}

	// VERIFICATION: Current should now be V2
	secretInfo = trackingData.Secrets["nova-config"]

	if secretInfo.CurrentHash != "hash-v2" {
		t.Errorf("CurrentHash = %s, want hash-v2", secretInfo.CurrentHash)
	}

	if secretInfo.CurrentResourceVersion != "200" {
		t.Errorf("CurrentResourceVersion = %s, want 200", secretInfo.CurrentResourceVersion)
	}

	if len(secretInfo.NodesWithCurrent) != 2 {
		t.Errorf("NodesWithCurrent count = %d, want 2", len(secretInfo.NodesWithCurrent))
	}

	if secretInfo.PreviousHash != "" {
		t.Errorf("PreviousHash should be cleared, got %s", secretInfo.PreviousHash)
	}

	// Summary should show all nodes updated
	summary := computeDeploymentSummary(trackingData, 2, "test-configmap")

	if !summary.AllNodesUpdated {
		t.Error("AllNodesUpdated should be true after deployment completes")
	}

	if summary.UpdatedNodes != 2 {
		t.Errorf("UpdatedNodes = %d, want 2", summary.UpdatedNodes)
	}
}

// TestSecretDeletedDuringDeploymentProcessing tests that if a secret in
// deployment.Status.SecretHashes is deleted from cluster, we skip it gracefully
func TestSecretDeletedDuringDeploymentProcessing(t *testing.T) {
	trackingData := &SecretTrackingData{
		Secrets: map[string]SecretVersionInfo{
			"existing-secret": {
				CurrentHash:      "hash-v1",
				ExpectedHash:     "hash-v1",
				NodesWithCurrent: []string{"compute-0"},
			},
		},
		NodeStatus: make(map[string]NodeSecretStatus),
	}

	// Simulate deployment processing where one secret exists, one doesn't
	// In real code, the secret fetch would return NotFound, we skip it, and continue
	// Here we just verify the logic handles partial updates correctly

	// Process existing secret (simulating same-version path)
	secretInfo := trackingData.Secrets["existing-secret"]
	secretInfo.NodesWithCurrent = []string{"compute-0", "compute-1"}
	trackingData.Secrets["existing-secret"] = secretInfo

	// Verify existing secret was updated
	if len(trackingData.Secrets["existing-secret"].NodesWithCurrent) != 2 {
		t.Error("Existing secret should have been updated with both nodes")
	}

	// Verify we don't have the deleted secret in tracking
	if _, exists := trackingData.Secrets["deleted-secret"]; exists {
		t.Error("Deleted secret should not be in tracking")
	}
}

// TestEmptyDeploymentSecretHashes tests that if deployment.Status.SecretHashes is empty,
// we handle it gracefully (defensive validation)
func TestEmptyDeploymentSecretHashes(t *testing.T) {
	trackingData := &SecretTrackingData{
		Secrets:    make(map[string]SecretVersionInfo),
		NodeStatus: make(map[string]NodeSecretStatus),
	}

	// Simulate empty deployment.Status.SecretHashes
	// In real code, we return early with nil error
	// Here we verify tracking is unchanged
	secretHashes := map[string]string{} // Empty!

	if len(secretHashes) == 0 {
		// Early return - tracking should be unchanged
		if len(trackingData.Secrets) != 0 {
			t.Error("Tracking should remain empty when deployment has no secret hashes")
		}
	}
}

// TestHashMatchButResourceVersionDiffers tests scenario where K8s updates
// secret metadata without changing content (hash same, ResourceVersion different)
func TestHashMatchButResourceVersionDiffers(t *testing.T) {
	trackingData := &SecretTrackingData{
		Secrets: map[string]SecretVersionInfo{
			"test-secret": {
				CurrentHash:             "hash-abc",
				CurrentResourceVersion:  "100",
				CurrentGeneration:       1,
				ExpectedHash:            "hash-abc",
				ExpectedResourceVersion: "100",
				ExpectedGeneration:      1,
				NodesWithCurrent:        []string{"compute-0", "compute-1"},
			},
		},
		NodeStatus: make(map[string]NodeSecretStatus),
	}

	// Simulate K8s updating ResourceVersion but content unchanged
	// (can happen with annotation updates, etc.)
	newResourceVersion := "101"
	newGeneration := int64(2)
	hashUnchanged := "hash-abc"

	// Update Expected to reflect cluster state (drift detection would do this)
	secretInfo := trackingData.Secrets["test-secret"]
	secretInfo.ExpectedResourceVersion = newResourceVersion
	secretInfo.ExpectedGeneration = newGeneration
	secretInfo.ExpectedHash = hashUnchanged
	trackingData.Secrets["test-secret"] = secretInfo

	// Check drift using hash comparison
	hasDrift := secretInfo.CurrentHash != secretInfo.ExpectedHash

	if hasDrift {
		t.Error("No drift should be detected when hashes match, even if ResourceVersion differs")
	}

	// This validates our hash-based approach correctly ignores metadata-only changes
}

// TestSecretRotationBetweenDeploymentAndReconciliation tests the race condition
// scenario where secret rotates after deployment completes but before reconciliation
func TestSecretRotationBetweenDeploymentAndReconciliation(t *testing.T) {
	trackingData := &SecretTrackingData{
		Secrets: map[string]SecretVersionInfo{
			"nova-config": {
				CurrentHash:             "hash-v1",
				CurrentResourceVersion:  "100",
				CurrentGeneration:       1,
				ExpectedHash:            "hash-v1",
				ExpectedResourceVersion: "100",
				ExpectedGeneration:      1,
				NodesWithCurrent:        []string{"compute-0", "compute-1"},
			},
		},
		NodeStatus: make(map[string]NodeSecretStatus),
	}

	// Deployment completed with V2 (hash-v2)
	deploymentHash := "hash-v2"

	// But secret rotated to V3 in cluster before reconciliation
	clusterHash := "hash-v3"
	clusterResourceVersion := "300"

	secretInfo := trackingData.Secrets["nova-config"]

	// Deployment processing: compare Current vs deployment hash
	if secretInfo.CurrentHash != deploymentHash {
		// Rotation detected! Update Current to deployment version
		secretInfo.PreviousHash = secretInfo.CurrentHash
		secretInfo.CurrentHash = deploymentHash
		secretInfo.CurrentResourceVersion = "200" // Would come from deployment.Status if available
		secretInfo.NodesWithCurrent = []string{"compute-0", "compute-1"}
	}

	// Drift detection: compare Current vs cluster hash
	if secretInfo.CurrentHash != clusterHash {
		// Drift detected! Update Expected to cluster version
		secretInfo.ExpectedHash = clusterHash
		secretInfo.ExpectedResourceVersion = clusterResourceVersion
	}

	trackingData.Secrets["nova-config"] = secretInfo

	// Set up node status - both nodes have the current (deployed) version
	trackingData.NodeStatus = map[string]NodeSecretStatus{
		"compute-0": {AllSecretsUpdated: true},
		"compute-1": {AllSecretsUpdated: true},
	}

	// Verify final state
	if secretInfo.CurrentHash != "hash-v2" {
		t.Errorf("CurrentHash should be hash-v2 (what was deployed), got %s", secretInfo.CurrentHash)
	}

	if secretInfo.ExpectedHash != "hash-v3" {
		t.Errorf("ExpectedHash should be hash-v3 (cluster state), got %s", secretInfo.ExpectedHash)
	}

	// Drift should be detected
	summary := computeDeploymentSummary(trackingData, 2, "test-cm")
	if summary.AllNodesUpdated {
		t.Error("AllNodesUpdated should be false when drift detected (Current != Expected)")
	}

	// UpdatedNodes should be 0 when there's drift (nodes don't have expected version)
	if summary.UpdatedNodes != 0 {
		t.Errorf("UpdatedNodes should be 0 when drift detected, got %d", summary.UpdatedNodes)
	}

	// This validates hash-based approach correctly handles the race condition
}

// TestRotationWithTwoSeparateLimitedDeployments simulates the exact scenario:
// 1. Initial deployment covers all nodes with V1
// 2. Secret rotates to V2 in cluster
// 3. First deployment with AnsibleLimit targets compute-0, deploys V2
// 4. Second deployment with AnsibleLimit targets compute-1, deploys V2
// 5. Both nodes should accumulate in nodesWithCurrent, no duplicates in previous
func TestRotationWithTwoSeparateLimitedDeployments(t *testing.T) {
	// STEP 1: Initial state - all nodes have V1
	trackingData := &SecretTrackingData{
		Secrets: map[string]SecretVersionInfo{
			"nova-cell1-compute-config": {
				CurrentHash:             "hash-user5-v1",
				CurrentResourceVersion:  "15940100",
				CurrentGeneration:       0,
				ExpectedHash:            "hash-user5-v1",
				ExpectedResourceVersion: "15940100",
				ExpectedGeneration:      0,
				NodesWithCurrent:        []string{"edpm-compute-0", "edpm-compute-1"},
				LastChanged:             time.Now().Add(-1 * time.Hour),
			},
		},
		NodeStatus: map[string]NodeSecretStatus{
			"edpm-compute-0": {
				AllSecretsUpdated:  true,
				SecretsWithCurrent: []string{"nova-cell1-compute-config"},
			},
			"edpm-compute-1": {
				AllSecretsUpdated:  true,
				SecretsWithCurrent: []string{"nova-cell1-compute-config"},
			},
		},
	}

	// Verify initial state is clean
	summary := computeDeploymentSummary(trackingData, 2, "test-cm")
	if !summary.AllNodesUpdated {
		t.Error("Initial state: AllNodesUpdated should be true")
	}
	if summary.UpdatedNodes != 2 {
		t.Errorf("Initial state: UpdatedNodes = %d, want 2", summary.UpdatedNodes)
	}

	// STEP 2: Secret rotates to V2 (user switched from user5 to user6)
	// Cluster now has hash-user6-v2
	// Drift detection updates Expected
	secretInfo := trackingData.Secrets["nova-cell1-compute-config"]
	secretInfo.ExpectedHash = "hash-user6-v2"
	secretInfo.ExpectedResourceVersion = "15940207"
	secretInfo.ExpectedGeneration = 0
	trackingData.Secrets["nova-cell1-compute-config"] = secretInfo

	// Verify drift is detected
	summary = computeDeploymentSummary(trackingData, 2, "test-cm")
	if summary.AllNodesUpdated {
		t.Error("After rotation: AllNodesUpdated should be false (drift detected)")
	}
	if summary.UpdatedNodes != 0 {
		t.Errorf("After rotation: UpdatedNodes = %d, want 0 (drift detected)", summary.UpdatedNodes)
	}

	// STEP 3: First deployment "edpm-deployment" completes - full deployment
	// This triggers rotation detection because deployment hash != current hash
	// But let's say this was the OLD deployment that ran before user switched
	// Skip this for now, go straight to the limited deployments

	// Actually, let me simulate what the logs showed:
	// The "edpm-deployment" ran and rotated BACK from user6 to user5
	// Then c0-limit and c1-limit ran with user6

	// Simulating "edpm-deployment" processing (this seems to have rotated back?)
	// Let's skip to the relevant scenario

	// STEP 3: First limited deployment "edpm-deployment-c0-limit" completes
	// Targets only edpm-compute-0, has V2 (user6)
	deploymentHashV2 := "hash-user6-v2"
	coveredNodesC0 := []string{"edpm-compute-0"}

	secretInfo = trackingData.Secrets["nova-cell1-compute-config"]

	// Check if rotation (current hash != deployment hash)
	if secretInfo.CurrentHash != deploymentHashV2 {
		// ROTATION detected
		secretInfo.PreviousHash = secretInfo.CurrentHash
		secretInfo.PreviousResourceVersion = secretInfo.CurrentResourceVersion
		secretInfo.PreviousGeneration = secretInfo.CurrentGeneration
		secretInfo.NodesWithPrevious = secretInfo.NodesWithCurrent

		secretInfo.CurrentHash = deploymentHashV2
		secretInfo.CurrentResourceVersion = "15940207"
		secretInfo.CurrentGeneration = 0
		secretInfo.NodesWithCurrent = coveredNodesC0 // Only compute-0
		secretInfo.LastChanged = time.Now()
	}

	trackingData.Secrets["nova-cell1-compute-config"] = secretInfo

	// Verify state after first deployment
	secretInfo = trackingData.Secrets["nova-cell1-compute-config"]
	if len(secretInfo.NodesWithCurrent) != 1 {
		t.Errorf("After c0-limit: nodesWithCurrent count = %d, want 1", len(secretInfo.NodesWithCurrent))
	}
	if !slices.Contains(secretInfo.NodesWithCurrent, "edpm-compute-0") {
		t.Error("After c0-limit: edpm-compute-0 should be in nodesWithCurrent")
	}
	if len(secretInfo.NodesWithPrevious) != 2 {
		t.Errorf("After c0-limit: nodesWithPrevious count = %d, want 2", len(secretInfo.NodesWithPrevious))
	}

	// STEP 4: Second limited deployment "edpm-deployment-c1-limit" completes
	// Targets only edpm-compute-1, has V2 (user6)
	coveredNodesC1 := []string{"edpm-compute-1"}

	secretInfo = trackingData.Secrets["nova-cell1-compute-config"]

	// This should be SAME version path (current hash == deployment hash)
	if secretInfo.CurrentHash == deploymentHashV2 {
		// SAME version - accumulate nodes
		for _, node := range coveredNodesC1 {
			if !slices.Contains(secretInfo.NodesWithCurrent, node) {
				secretInfo.NodesWithCurrent = append(secretInfo.NodesWithCurrent, node)
			}

			// Remove from previous if it was there (node upgraded)
			if secretInfo.PreviousHash != "" {
				newPrevious := []string{}
				for _, prevNode := range secretInfo.NodesWithPrevious {
					if prevNode != node {
						newPrevious = append(newPrevious, prevNode)
					}
				}
				secretInfo.NodesWithPrevious = newPrevious
			}
		}

		// Clear previous version metadata if all nodes now have current version
		totalNodes := 2
		if len(secretInfo.NodesWithCurrent) == totalNodes && secretInfo.PreviousHash != "" {
			secretInfo.PreviousHash = ""
			secretInfo.PreviousResourceVersion = ""
			secretInfo.PreviousGeneration = 0
			secretInfo.NodesWithPrevious = []string{}
		}
	} else {
		t.Fatal("c1-limit should be SAME version path, but hash differs!")
	}

	trackingData.Secrets["nova-cell1-compute-config"] = secretInfo

	// VERIFY: Final state after both deployments
	secretInfo = trackingData.Secrets["nova-cell1-compute-config"]

	// Both nodes should be in nodesWithCurrent
	if len(secretInfo.NodesWithCurrent) != 2 {
		t.Errorf("After c1-limit: nodesWithCurrent count = %d, want 2", len(secretInfo.NodesWithCurrent))
	}
	if !slices.Contains(secretInfo.NodesWithCurrent, "edpm-compute-0") {
		t.Error("After c1-limit: edpm-compute-0 should be in nodesWithCurrent")
	}
	if !slices.Contains(secretInfo.NodesWithCurrent, "edpm-compute-1") {
		t.Error("After c1-limit: edpm-compute-1 should be in nodesWithCurrent")
	}

	// No nodes should be in nodesWithPrevious
	if len(secretInfo.NodesWithPrevious) != 0 {
		t.Errorf("After c1-limit: nodesWithPrevious should be empty, got %v", secretInfo.NodesWithPrevious)
	}

	// PreviousHash should be cleared
	if secretInfo.PreviousHash != "" {
		t.Errorf("After c1-limit: previousHash should be cleared, got %s", secretInfo.PreviousHash)
	}

	// Verify no node is in BOTH lists
	for _, node := range secretInfo.NodesWithCurrent {
		if slices.Contains(secretInfo.NodesWithPrevious, node) {
			t.Errorf("Node %s is in BOTH nodesWithCurrent and nodesWithPrevious!", node)
		}
	}

	// Update node status
	for _, nodeName := range []string{"edpm-compute-0", "edpm-compute-1"} {
		nodeStatus := NodeSecretStatus{
			AllSecretsUpdated:   true,
			SecretsWithCurrent:  []string{},
			SecretsWithPrevious: []string{},
		}

		for secretName, sInfo := range trackingData.Secrets {
			if slices.Contains(sInfo.NodesWithCurrent, nodeName) {
				nodeStatus.SecretsWithCurrent = append(nodeStatus.SecretsWithCurrent, secretName)
			} else if slices.Contains(sInfo.NodesWithPrevious, nodeName) {
				nodeStatus.SecretsWithPrevious = append(nodeStatus.SecretsWithPrevious, secretName)
				nodeStatus.AllSecretsUpdated = false
			} else {
				nodeStatus.AllSecretsUpdated = false
			}
		}

		trackingData.NodeStatus[nodeName] = nodeStatus
	}

	// Verify summary
	summary = computeDeploymentSummary(trackingData, 2, "test-cm")
	if !summary.AllNodesUpdated {
		t.Error("Final: AllNodesUpdated should be true after both limited deployments complete")
	}
	if summary.UpdatedNodes != 2 {
		t.Errorf("Final: UpdatedNodes = %d, want 2", summary.UpdatedNodes)
	}
}

// TestStaleDeploymentDoesNotFlipFlop tests the fix for the scenario where
// an old deployment (completed before secret rotation) is processed after
// newer deployments, causing flip-flop in tracking state.
func TestStaleDeploymentDoesNotFlipFlop(t *testing.T) {
	scheme := runtime.NewScheme()
	_ = corev1.AddToScheme(scheme)

	// Create current cluster secret (new version - user6)
	clusterSecret := &corev1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:            "nova-cell1-compute-config",
			Namespace:       "test-ns",
			ResourceVersion: "15940207",
		},
		Data: map[string][]byte{
			"rabbitmq_user_name": []byte("nova-cell1-transport-user6-user"),
		},
	}
	clusterHash, _ := secret.Hash(clusterSecret)

	// Create old secret for comparison (what old deployment saw - user5)
	oldSecret := &corev1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:            "nova-cell1-compute-config",
			Namespace:       "test-ns",
			ResourceVersion: "15940100", // Different RV
		},
		Data: map[string][]byte{
			"rabbitmq_user_name": []byte("nova-cell1-transport-user5-user"),
		},
	}
	oldHash, _ := secret.Hash(oldSecret)

	// Verify hashes differ
	if clusterHash == oldHash {
		t.Fatal("Test setup: hashes should differ")
	}

	client := fake.NewClientBuilder().
		WithScheme(scheme).
		WithRuntimeObjects(clusterSecret).
		Build()

	r := &OpenStackDataPlaneNodeSetReconciler{
		Client: client,
		Scheme: scheme,
	}

	// Tracking data after new deployments (c0-limit, c1-limit) completed with user6
	trackingData := &SecretTrackingData{
		Secrets: map[string]SecretVersionInfo{
			"nova-cell1-compute-config": {
				CurrentHash:             clusterHash, // Current = user6 hash
				CurrentResourceVersion:  "15940207",
				ExpectedHash:            clusterHash, // Expected also = user6 (no drift)
				ExpectedResourceVersion: "15940207",
				NodesWithCurrent:        []string{"edpm-compute-0", "edpm-compute-1"},
			},
		},
		NodeStatus: map[string]NodeSecretStatus{
			"edpm-compute-0": {AllSecretsUpdated: true},
			"edpm-compute-1": {AllSecretsUpdated: true},
		},
	}

	// Verify summary before processing stale deployment
	summary := computeDeploymentSummary(trackingData, 2, "test-cm")
	if !summary.AllNodesUpdated {
		t.Error("Before stale deployment: AllNodesUpdated should be true")
	}
	if summary.UpdatedNodes != 2 {
		t.Errorf("Before stale deployment: UpdatedNodes = %d, want 2", summary.UpdatedNodes)
	}

	// Now process old deployment with stale hash (user5)
	staleDeployment := &dataplanev1.OpenStackDataPlaneDeployment{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "edpm-deployment",
			Namespace: "test-ns",
		},
		Spec: dataplanev1.OpenStackDataPlaneDeploymentSpec{
			NodeSets: []string{"openstack-edpm-ipam"},
		},
		Status: dataplanev1.OpenStackDataPlaneDeploymentStatus{
			SecretHashes: map[string]string{
				"nova-cell1-compute-config": oldHash, // Stale! (user5)
			},
		},
	}

	// Simulate what updateSecretDeploymentTracking does with the fix:
	// 1. Get deployment hash
	deploymentHash := staleDeployment.Status.SecretHashes["nova-cell1-compute-config"]

	// 2. Fetch cluster secret
	fetchedSecret := &corev1.Secret{}
	err := r.Get(context.Background(), types.NamespacedName{
		Name:      "nova-cell1-compute-config",
		Namespace: "test-ns",
	}, fetchedSecret)
	if err != nil {
		t.Fatalf("Failed to fetch secret: %v", err)
	}

	// 3. Compute cluster hash
	fetchedHash, err := secret.Hash(fetchedSecret)
	if err != nil {
		t.Fatalf("Failed to compute hash: %v", err)
	}

	// 4. Check if deployment is stale (THE FIX)
	isStale := fetchedHash != deploymentHash

	// VERIFY: With the fix, stale deployment should be SKIPPED
	if !isStale {
		t.Error("Test setup error: deployment should be stale (cluster hash != deployment hash)")
	}

	// 5. With the NEW fix: SKIP stale deployments entirely
	// Don't process tracking at all for this deployment
	// trackingData remains unchanged

	// VERIFY: Tracking state is NOT modified (deployment was skipped)
	secretInfo := trackingData.Secrets["nova-cell1-compute-config"]
	if secretInfo.CurrentHash != clusterHash {
		t.Error("Tracking CurrentHash should remain unchanged (stale deployment skipped)")
	}

	// Verify stale deployment hash was NOT used
	if secretInfo.CurrentHash == oldHash {
		t.Error("Tracking should NOT have stale deployment hash (deployment was skipped)")
	}

	// Verify tracking state remains correct
	summary = computeDeploymentSummary(trackingData, 2, "test-cm")
	if !summary.AllNodesUpdated {
		t.Error("After stale deployment: AllNodesUpdated should still be true (no flip-flop)")
	}
	if summary.UpdatedNodes != 2 {
		t.Errorf("After stale deployment: UpdatedNodes = %d, want 2 (no flip-flop)", summary.UpdatedNodes)
	}
}

// TestSingleNodeDeploymentDuringRotation tests the scenario where:
// 1. Both nodes have user5 (old credentials)
// 2. Secret rotates to user6 in cluster
// 3. Deployment runs with AnsibleLimit targeting only one node (edpm-compute-0)
// 4. Status should show allNodesUpdated=false, updatedNodes=1
// 5. This prevents premature credential deletion (edpm-compute-1 still has old creds)
func TestSingleNodeDeploymentDuringRotation(t *testing.T) {
	// STEP 1: Initial state - both nodes have V1 (user5)
	trackingData := &SecretTrackingData{
		Secrets: map[string]SecretVersionInfo{
			"nova-cell1-compute-config": {
				CurrentHash:             "hash-user5",
				CurrentResourceVersion:  "15940100",
				CurrentGeneration:       0,
				ExpectedHash:            "hash-user5",
				ExpectedResourceVersion: "15940100",
				ExpectedGeneration:      0,
				NodesWithCurrent:        []string{"edpm-compute-0", "edpm-compute-1"},
				LastChanged:             time.Now().Add(-1 * time.Hour),
			},
		},
		NodeStatus: map[string]NodeSecretStatus{
			"edpm-compute-0": {
				AllSecretsUpdated:  true,
				SecretsWithCurrent: []string{"nova-cell1-compute-config"},
			},
			"edpm-compute-1": {
				AllSecretsUpdated:  true,
				SecretsWithCurrent: []string{"nova-cell1-compute-config"},
			},
		},
	}

	// Verify initial state is clean
	summary := computeDeploymentSummary(trackingData, 2, "test-cm")
	if !summary.AllNodesUpdated {
		t.Error("Initial state: AllNodesUpdated should be true")
	}
	if summary.UpdatedNodes != 2 {
		t.Errorf("Initial state: UpdatedNodes = %d, want 2", summary.UpdatedNodes)
	}

	// STEP 2: Secret rotates to V2 (user6) in cluster
	// Drift detection updates Expected hash
	secretInfo := trackingData.Secrets["nova-cell1-compute-config"]
	secretInfo.ExpectedHash = "hash-user6"
	secretInfo.ExpectedResourceVersion = "15940207"
	secretInfo.ExpectedGeneration = 0
	trackingData.Secrets["nova-cell1-compute-config"] = secretInfo

	// Verify drift is detected
	summary = computeDeploymentSummary(trackingData, 2, "test-cm")
	if summary.AllNodesUpdated {
		t.Error("After rotation: AllNodesUpdated should be false (drift detected)")
	}
	if summary.UpdatedNodes != 0 {
		t.Errorf("After rotation: UpdatedNodes = %d, want 0 (drift detected)", summary.UpdatedNodes)
	}

	// STEP 3: Deployment with AnsibleLimit=edpm-compute-0 completes
	// This targets ONLY edpm-compute-0, not both nodes
	deploymentHashV2 := "hash-user6"
	coveredNodes := []string{"edpm-compute-0"} // Only one node!

	secretInfo = trackingData.Secrets["nova-cell1-compute-config"]

	// Check if rotation (current hash != deployment hash)
	if secretInfo.CurrentHash != deploymentHashV2 {
		// ROTATION detected
		secretInfo.PreviousHash = secretInfo.CurrentHash
		secretInfo.PreviousResourceVersion = secretInfo.CurrentResourceVersion
		secretInfo.PreviousGeneration = secretInfo.CurrentGeneration
		secretInfo.NodesWithPrevious = secretInfo.NodesWithCurrent

		secretInfo.CurrentHash = deploymentHashV2
		secretInfo.CurrentResourceVersion = "15940207"
		secretInfo.CurrentGeneration = 0
		secretInfo.NodesWithCurrent = coveredNodes // ONLY edpm-compute-0
		secretInfo.LastChanged = time.Now()

		// Don't clear previous - not all nodes updated
		// len(NodesWithCurrent) = 1, totalNodes = 2, so condition is false
	}

	// Update Expected to match cluster (drift detection would do this)
	secretInfo.ExpectedHash = deploymentHashV2
	secretInfo.ExpectedResourceVersion = "15940207"
	secretInfo.ExpectedGeneration = 0

	trackingData.Secrets["nova-cell1-compute-config"] = secretInfo

	// Update per-node status
	allNodes := []string{"edpm-compute-0", "edpm-compute-1"}
	for _, nodeName := range allNodes {
		nodeStatus := NodeSecretStatus{
			AllSecretsUpdated:   true,
			SecretsWithCurrent:  []string{},
			SecretsWithPrevious: []string{},
		}

		for secretName, sInfo := range trackingData.Secrets {
			if slices.Contains(sInfo.NodesWithCurrent, nodeName) {
				nodeStatus.SecretsWithCurrent = append(nodeStatus.SecretsWithCurrent, secretName)
			} else if slices.Contains(sInfo.NodesWithPrevious, nodeName) {
				nodeStatus.SecretsWithPrevious = append(nodeStatus.SecretsWithPrevious, secretName)
				nodeStatus.AllSecretsUpdated = false
			} else {
				nodeStatus.AllSecretsUpdated = false
			}
		}

		trackingData.NodeStatus[nodeName] = nodeStatus
	}

	// VERIFY: State after single-node deployment
	secretInfo = trackingData.Secrets["nova-cell1-compute-config"]

	// Only edpm-compute-0 should be in NodesWithCurrent
	if len(secretInfo.NodesWithCurrent) != 1 {
		t.Errorf("After single-node deployment: NodesWithCurrent count = %d, want 1", len(secretInfo.NodesWithCurrent))
	}
	if !slices.Contains(secretInfo.NodesWithCurrent, "edpm-compute-0") {
		t.Error("After single-node deployment: edpm-compute-0 should be in NodesWithCurrent")
	}
	if slices.Contains(secretInfo.NodesWithCurrent, "edpm-compute-1") {
		t.Error("After single-node deployment: edpm-compute-1 should NOT be in NodesWithCurrent")
	}

	// edpm-compute-1 should still be in NodesWithPrevious
	if len(secretInfo.NodesWithPrevious) != 2 {
		t.Errorf("After single-node deployment: NodesWithPrevious count = %d, want 2", len(secretInfo.NodesWithPrevious))
	}
	if !slices.Contains(secretInfo.NodesWithPrevious, "edpm-compute-1") {
		t.Error("After single-node deployment: edpm-compute-1 should be in NodesWithPrevious (still has old secret)")
	}

	// PreviousHash should NOT be cleared (not all nodes updated)
	if secretInfo.PreviousHash == "" {
		t.Error("After single-node deployment: PreviousHash should NOT be cleared (edpm-compute-1 still has old version)")
	}

	// Verify per-node status
	// edpm-compute-0: should have all secrets updated
	if !trackingData.NodeStatus["edpm-compute-0"].AllSecretsUpdated {
		t.Error("After single-node deployment: edpm-compute-0 should have AllSecretsUpdated=true")
	}

	// edpm-compute-1: should NOT have all secrets updated (still has previous version)
	if trackingData.NodeStatus["edpm-compute-1"].AllSecretsUpdated {
		t.Error("After single-node deployment: edpm-compute-1 should have AllSecretsUpdated=false (still has old secret)")
	}

	// Verify summary status
	summary = computeDeploymentSummary(trackingData, 2, "test-cm")

	// CRITICAL: AllNodesUpdated should be FALSE (edpm-compute-1 still has old secret)
	if summary.AllNodesUpdated {
		t.Error("After single-node deployment: AllNodesUpdated should be FALSE (edpm-compute-1 not updated yet)")
	}

	// CRITICAL: UpdatedNodes should be 1, not 2 (only edpm-compute-0 has new secret)
	if summary.UpdatedNodes != 1 {
		t.Errorf("After single-node deployment: UpdatedNodes = %d, want 1 (only edpm-compute-0 updated)", summary.UpdatedNodes)
	}

	// This is the key test: credential deletion should be BLOCKED
	// because AllNodesUpdated=false, meaning edpm-compute-1 still needs old credentials
}

// TestSingleNodeDeploymentWithMultipleSecrets tests the scenario where:
// 1. There are MULTIPLE secrets being tracked
// 2. One secret (nova-cell1-compute-config) rotates and deploys to single node
// 3. Another secret (nova-metadata-config) has both nodes covered
// 4. Bug: AllSecretsUpdated might incorrectly be true for edpm-compute-1
//    because it has the non-rotated secret, ignoring that it's missing the rotated one
func TestSingleNodeDeploymentWithMultipleSecrets(t *testing.T) {
	// STEP 1: Initial state - both nodes have both secrets at V1
	trackingData := &SecretTrackingData{
		Secrets: map[string]SecretVersionInfo{
			"nova-cell1-compute-config": {
				CurrentHash:             "hash-user5",
				CurrentResourceVersion:  "100",
				CurrentGeneration:       0,
				ExpectedHash:            "hash-user5",
				ExpectedResourceVersion: "100",
				ExpectedGeneration:      0,
				NodesWithCurrent:        []string{"edpm-compute-0", "edpm-compute-1"},
				LastChanged:             time.Now().Add(-1 * time.Hour),
			},
			"nova-metadata-config": {
				CurrentHash:             "metadata-v1",
				CurrentResourceVersion:  "200",
				CurrentGeneration:       0,
				ExpectedHash:            "metadata-v1",
				ExpectedResourceVersion: "200",
				ExpectedGeneration:      0,
				NodesWithCurrent:        []string{"edpm-compute-0", "edpm-compute-1"},
				LastChanged:             time.Now().Add(-1 * time.Hour),
			},
		},
		NodeStatus: map[string]NodeSecretStatus{
			"edpm-compute-0": {
				AllSecretsUpdated:  true,
				SecretsWithCurrent: []string{"nova-cell1-compute-config", "nova-metadata-config"},
			},
			"edpm-compute-1": {
				AllSecretsUpdated:  true,
				SecretsWithCurrent: []string{"nova-cell1-compute-config", "nova-metadata-config"},
			},
		},
	}

	// STEP 2: Only nova-cell1-compute-config rotates to user6
	secretInfo := trackingData.Secrets["nova-cell1-compute-config"]
	secretInfo.ExpectedHash = "hash-user6"
	secretInfo.ExpectedResourceVersion = "101"
	trackingData.Secrets["nova-cell1-compute-config"] = secretInfo

	// STEP 3: Deployment with AnsibleLimit=edpm-compute-0 completes
	// It includes BOTH secrets, but only covers edpm-compute-0
	coveredNodes := []string{"edpm-compute-0"}

	// Process nova-cell1-compute-config (rotated)
	secretInfo = trackingData.Secrets["nova-cell1-compute-config"]
	if secretInfo.CurrentHash != "hash-user6" {
		// ROTATION
		secretInfo.PreviousHash = secretInfo.CurrentHash
		secretInfo.PreviousResourceVersion = secretInfo.CurrentResourceVersion
		secretInfo.PreviousGeneration = secretInfo.CurrentGeneration
		secretInfo.NodesWithPrevious = secretInfo.NodesWithCurrent

		secretInfo.CurrentHash = "hash-user6"
		secretInfo.CurrentResourceVersion = "101"
		secretInfo.CurrentGeneration = 0
		secretInfo.NodesWithCurrent = coveredNodes // Only edpm-compute-0
		secretInfo.LastChanged = time.Now()
	}
	secretInfo.ExpectedHash = "hash-user6"
	trackingData.Secrets["nova-cell1-compute-config"] = secretInfo

	// Process nova-metadata-config (NOT rotated, same version)
	// But deployment only covered edpm-compute-0, so we need to check if this
	// secret was also in the deployment
	// For this test, assume it WAS in the deployment with same hash
	metadataInfo := trackingData.Secrets["nova-metadata-config"]
	if metadataInfo.CurrentHash == "metadata-v1" {
		// SAME version - accumulate nodes
		// But wait - if this is the FIRST deployment after rotation,
		// and it only targets edpm-compute-0, should we set NodesWithCurrent
		// to ONLY edpm-compute-0, or keep both nodes?

		// The bug might be here! If we don't update NodesWithCurrent for
		// non-rotated secrets when using AnsibleLimit, then edpm-compute-1
		// still shows as having this secret, making AllSecretsUpdated=true
		// for the wrong reason.

		// Let's test the WRONG behavior (the bug):
		// NodesWithCurrent is NOT updated, stays as both nodes
		// (This would be the bug - should only be covered nodes)
	}
	// Don't update - this is the potential bug
	// trackingData.Secrets["nova-metadata-config"] = metadataInfo (unchanged)

	// Update per-node status
	allNodes := []string{"edpm-compute-0", "edpm-compute-1"}
	for _, nodeName := range allNodes {
		nodeStatus := NodeSecretStatus{
			AllSecretsUpdated:   true,
			SecretsWithCurrent:  []string{},
			SecretsWithPrevious: []string{},
		}

		for secretName, sInfo := range trackingData.Secrets {
			if slices.Contains(sInfo.NodesWithCurrent, nodeName) {
				nodeStatus.SecretsWithCurrent = append(nodeStatus.SecretsWithCurrent, secretName)
			} else if slices.Contains(sInfo.NodesWithPrevious, nodeName) {
				nodeStatus.SecretsWithPrevious = append(nodeStatus.SecretsWithPrevious, secretName)
				nodeStatus.AllSecretsUpdated = false
			} else {
				nodeStatus.AllSecretsUpdated = false
			}
		}

		trackingData.NodeStatus[nodeName] = nodeStatus
	}

	// VERIFY: Check node status
	// edpm-compute-0:
	//   - nova-cell1-compute-config: in NodesWithCurrent ✓
	//   - nova-metadata-config: in NodesWithCurrent ✓
	//   - AllSecretsUpdated: TRUE ✓

	// edpm-compute-1:
	//   - nova-cell1-compute-config: in NodesWithPrevious (NOT current!)
	//   - nova-metadata-config: in NodesWithCurrent (BUG if unchanged!)
	//   - If metadata shows as current: Secret exists but it's WRONG version?
	//   - Or is the version the same but the deployment didn't actually run on this node?

	// Actually, I think I'm confusing myself. Let me reconsider.
	// If nova-metadata-config didn't rotate, and both nodes already had it,
	// then both nodes DO have the current version of that secret.
	// The issue is only with nova-cell1-compute-config.

	// So edpm-compute-1 should have:
	//   - nova-cell1-compute-config: in NodesWithPrevious → AllSecretsUpdated = FALSE
	//   - nova-metadata-config: in NodesWithCurrent
	//   - Overall: AllSecretsUpdated = FALSE (correct!)

	if trackingData.NodeStatus["edpm-compute-1"].AllSecretsUpdated {
		t.Error("edpm-compute-1 should have AllSecretsUpdated=false (missing rotated secret)")
	}

	summary := computeDeploymentSummary(trackingData, 2, "test-cm")
	if summary.AllNodesUpdated {
		t.Error("AllNodesUpdated should be false (edpm-compute-1 missing rotated secret)")
	}
	if summary.UpdatedNodes != 1 {
		t.Errorf("UpdatedNodes = %d, want 1", summary.UpdatedNodes)
	}
}

// TestStaleFullDeploymentAfterLimitedDeployment reproduces the bug where:
// 1. Limited deployment (c0-limit) completes with new version, only edpm-compute-0
// 2. Stale full deployment (edpm-deployment-full) is processed after
// 3. Stale deployment uses cluster hash (our fix), so it's "same version"
// 4. BUG: It accumulates ALL nodes, incorrectly marking edpm-compute-1 as updated
func TestStaleFullDeploymentAfterLimitedDeployment(t *testing.T) {
	// STEP 1: Both nodes have user5 initially
	trackingData := &SecretTrackingData{
		Secrets: map[string]SecretVersionInfo{
			"nova-cell1-compute-config": {
				CurrentHash:             "hash-user5",
				CurrentResourceVersion:  "100",
				CurrentGeneration:       0,
				ExpectedHash:            "hash-user5",
				ExpectedResourceVersion: "100",
				ExpectedGeneration:      0,
				NodesWithCurrent:        []string{"edpm-compute-0", "edpm-compute-1"},
				LastChanged:             time.Now().Add(-2 * time.Hour),
			},
		},
		NodeStatus: map[string]NodeSecretStatus{
			"edpm-compute-0": {
				AllSecretsUpdated:  true,
				SecretsWithCurrent: []string{"nova-cell1-compute-config"},
			},
			"edpm-compute-1": {
				AllSecretsUpdated:  true,
				SecretsWithCurrent: []string{"nova-cell1-compute-config"},
			},
		},
	}

	totalNodes := 2

	// STEP 2: Secret rotates to user6
	secretInfo := trackingData.Secrets["nova-cell1-compute-config"]
	secretInfo.ExpectedHash = "hash-user6"
	secretInfo.ExpectedResourceVersion = "101"
	trackingData.Secrets["nova-cell1-compute-config"] = secretInfo

	// STEP 3: c0-limit deployment completes (AnsibleLimit=edpm-compute-0, has user6)
	// This is the NEW deployment the user triggered
	deploymentHashC0 := "hash-user6"
	coveredNodesC0 := []string{"edpm-compute-0"}

	secretInfo = trackingData.Secrets["nova-cell1-compute-config"]
	if secretInfo.CurrentHash != deploymentHashC0 {
		// ROTATION detected
		secretInfo.PreviousHash = secretInfo.CurrentHash
		secretInfo.PreviousResourceVersion = secretInfo.CurrentResourceVersion
		secretInfo.PreviousGeneration = secretInfo.CurrentGeneration
		secretInfo.NodesWithPrevious = secretInfo.NodesWithCurrent

		secretInfo.CurrentHash = deploymentHashC0
		secretInfo.CurrentResourceVersion = "101"
		secretInfo.CurrentGeneration = 0
		secretInfo.NodesWithCurrent = coveredNodesC0 // Only edpm-compute-0
		secretInfo.LastChanged = time.Now()
	}
	secretInfo.ExpectedHash = deploymentHashC0
	trackingData.Secrets["nova-cell1-compute-config"] = secretInfo

	// Verify state after c0-limit
	if len(secretInfo.NodesWithCurrent) != 1 {
		t.Fatalf("After c0-limit: NodesWithCurrent should be 1, got %d", len(secretInfo.NodesWithCurrent))
	}

	// STEP 4: Now process STALE deployment-full (AnsibleLimit=none, has OLD user5)
	// This deployment completed BEFORE rotation but is processed AFTER c0-limit
	// in the same reconciliation loop
	deploymentHashFull := "hash-user5" // OLD hash
	clusterHash := "hash-user6"        // Current cluster has NEW hash

	// With the FIX: Skip stale deployments entirely
	if clusterHash != deploymentHashFull {
		// Deployment is stale - SKIP it completely
		// Don't update tracking at all
		// This is the FIX - we no longer try to use cluster hash and accumulate nodes
	} else {
		// Not stale, would process normally
		t.Fatal("Test setup error: deployment should be stale")
	}

	// State should remain unchanged from step 3
	secretInfo = trackingData.Secrets["nova-cell1-compute-config"]

	// VERIFY THE FIX: State should remain unchanged from c0-limit
	// Only edpm-compute-0 should be in NodesWithCurrent
	if len(secretInfo.NodesWithCurrent) != 1 {
		t.Errorf("After skipping stale deployment: NodesWithCurrent count = %d, want 1", len(secretInfo.NodesWithCurrent))
	}
	if !slices.Contains(secretInfo.NodesWithCurrent, "edpm-compute-0") {
		t.Error("After skipping stale deployment: edpm-compute-0 should be in NodesWithCurrent")
	}
	if slices.Contains(secretInfo.NodesWithCurrent, "edpm-compute-1") {
		t.Error("After skipping stale deployment: edpm-compute-1 should NOT be in NodesWithCurrent (wasn't deployed to)")
	}

	// Update per-node status
	allNodes := []string{"edpm-compute-0", "edpm-compute-1"}
	for _, nodeName := range allNodes {
		nodeStatus := NodeSecretStatus{
			AllSecretsUpdated:   true,
			SecretsWithCurrent:  []string{},
			SecretsWithPrevious: []string{},
		}

		for secretName, sInfo := range trackingData.Secrets {
			if slices.Contains(sInfo.NodesWithCurrent, nodeName) {
				nodeStatus.SecretsWithCurrent = append(nodeStatus.SecretsWithCurrent, secretName)
			} else if slices.Contains(sInfo.NodesWithPrevious, nodeName) {
				nodeStatus.SecretsWithPrevious = append(nodeStatus.SecretsWithPrevious, secretName)
				nodeStatus.AllSecretsUpdated = false
			} else {
				nodeStatus.AllSecretsUpdated = false
			}
		}

		trackingData.NodeStatus[nodeName] = nodeStatus
	}

	// Verify per-node status is correct
	if !trackingData.NodeStatus["edpm-compute-0"].AllSecretsUpdated {
		t.Error("edpm-compute-0 should have AllSecretsUpdated=true")
	}
	if trackingData.NodeStatus["edpm-compute-1"].AllSecretsUpdated {
		t.Error("edpm-compute-1 should have AllSecretsUpdated=false (still on previous version)")
	}

	// Check summary - should correctly show only 1 node updated
	summary := computeDeploymentSummary(trackingData, totalNodes, "test-cm")
	if summary.AllNodesUpdated {
		t.Error("AllNodesUpdated should be false (edpm-compute-1 not deployed to)")
	}
	if summary.UpdatedNodes != 1 {
		t.Errorf("UpdatedNodes = %d, want 1 (only edpm-compute-0 deployed)", summary.UpdatedNodes)
	}

	// CRITICAL: Credential deletion should be BLOCKED
	// because only 1 of 2 nodes has the new version
}
